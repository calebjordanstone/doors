View(transition_matrix)
transition_counts <- colSums(as.matrix(transition_matrix > 0)+0)
transition_counts
transition_counts <- mean(transition_counts[transition_counts != 0])
transition_counts
transition_weights <- colSums(transition_matrix)/colMax(transition_matrix)
transition_weights
colSums(transition_matrix)
colMax(transition_matrix)
sapply(transition_matrix,max,na.rm = TRUE)
sapply?
?sapply
sapply(transition_matrix, max, na.rm = TRUE, simplify = "array")
sapply(data.frame(transition_matrix), max, na.rm = TRUE, simplify = "array")
sapply(data.frame(transition_matrix), max, na.rm = TRUE, simplify = TRUE)
sapply(data.frame(transition_matrix), max, na.rm = TRUE)
data.matrix(sapply(data.frame(transition_matrix), max, na.rm = TRUE))
# lydia barnes created december 2023
library(tidyverse)
library(dplyr)
library(tidyjson)
library(stringr)
library(readr)
library(ggforce)
data_path <- '/Users/lydiabarnes/Documents/academe/data/upPer/behaviour_main'
project_path <- '/Users/lydiabarnes/Documents/academe/projects/upPer/behaviour_main'
date = '20231215b'
source(file.path(project_path,'src','get_files.R'))
files <- get_files()
source(file.path(project_path,'src','get_hits.R'))
source(file.path(project_path,'src','extract.R'))
fnl_u <- file.path(project_path,'results',paste(date,'untimed_t-all.csv',sep = "_"))
fnl_t <- file.path(project_path,'results',paste(date,'timed_t-all.csv',sep = "_"))
fnl_u <- file.path(project_path,'results',paste(date,'untimed_t-all.csv',sep = "_"))
fnl_t <- file.path(project_path,'results',paste(date,'timed_t-all.csv',sep = "_"))
untimed <- read.csv(fnl_u)
date = '20231215'
fnl_u <- file.path(project_path,'results',paste(date,'untimed_t-all.csv',sep = "_"))
fnl_t <- file.path(project_path,'results',paste(date,'timed_t-all.csv',sep = "_"))
untimed <- read.csv(fnl_u)
timed <- read.csv(fnl_t)
project_path <- '/Users/lydiabarnes/Documents/academe/projects/upPer/behaviour_main'
version <- date
results_u <- untimed %>%
group_by(subject,phase,image_type,disambiguated) %>%
summarise(
accuracy = mean(correct),
rt = mean(rt),
h1 = sum(hit1)/(sum(miss1)+sum(hit1)), #when real image was present, how likely were they to respond 'real'?
fa1 = sum(false_alarm1)/(sum(absent1)+sum(false_alarm1)), #when real image was absent, how likely were they to respond 'real'?
h2 = sum(hit2)/(sum(miss2)+sum(hit2)),
fa2 = sum(false_alarm2)/(sum(absent2)+sum(false_alarm2)),
h3 = sum(hit3)/(sum(miss3)+sum(hit3)),
fa3 = sum(false_alarm3)/(sum(absent3)+sum(false_alarm3)),
confidence = mean(confidence)
)
View(results_u)
h1 <- array(unlist(results_u$h1))
fa1 <- array(unlist(results_u$fa1))
results_u$fa1 <- as.numeric(tweak(fa1))
source(file.path(project_path,'src','tweak.R'))
results_u$fa1 <- as.numeric(tweak(fa1))
tweak <- function(h){
n = length(h)
idx <- which(h == 0.00)
h[idx] <- .5/n
idx <- which(is.nan(h))
h[idx] <- .5/n
return(h)
}
results_u$fa1 <- as.numeric(tweak(fa1))
View(results_u)
tweak <- function(h){
n = length(h)
browser()
idx <- which(h == 0.00)
h[idx] <- .5/n
idx <- which(is.nan(h))
h[idx] <- .5/n
return(h)
}
results_u$fa1 <- as.numeric(tweak(fa1))
ntrials <- results_u %>% summarise(n = count)
ntrials <- results_u %>% summarise(n = count())
ntrials <- results_u %>% summarise(n = n())
View(ntrials)
results_u <- untimed %>%
group_by(subject,phase,image_type,disambiguated) %>%
summarise(
n = n(),
accuracy = mean(correct),
rt = mean(rt),
h1 = sum(hit1)/(sum(miss1)+sum(hit1)), #when real image was present, how likely were they to respond 'real'?
fa1 = sum(false_alarm1)/(sum(absent1)+sum(false_alarm1)), #when real image was absent, how likely were they to respond 'real'?
h2 = sum(hit2)/(sum(miss2)+sum(hit2)),
fa2 = sum(false_alarm2)/(sum(absent2)+sum(false_alarm2)),
h3 = sum(hit3)/(sum(miss3)+sum(hit3)),
fa3 = sum(false_alarm3)/(sum(absent3)+sum(false_alarm3)),
confidence = mean(confidence)
)
View(results_u)
ntrials <- results_u %>% pull(n)
ntrials <- ntrials[1]
ntrials
ntrials <- results_u %>% pull(n)
ntrials <- ntrials[[1]]
results_u <- results_u %>% select(!n)
ntrials
View(results_u)
tweak <- function(fa,n){
idx <- which(fa == 0.00)
fa[idx] <- .5/n
idx <- which(is.nan(fa))
fa[idx] <- .5/n
return(fa)
}
fa1 <- array(unlist(results_u$fa1))
results_u$fa1 <- as.numeric(tweak(fa1,ntrials))
results_u$dprime1 <- qnorm(results_u$h1) - qnorm(results_u$fa1) #usually convert hit rate and false alarm rate to zscores before subtracting
fa2 <- array(unlist(results_u$fa2))
results_u$fa2 <- as.numeric(tweak(fa2,ntrials))
results_u$dprime2 <- qnorm(results_u$h2) - qnorm(results_u$fa2)
fa3 <- array(unlist(results_u$fa3))
results_u$fa3 <- as.numeric(tweak(fa3,ntrials))
results_u$dprime3 <- qnorm(results_u$h3) - qnorm(results_u$fa3)
# lydia barnes, june 2024
# adjusts a false-alarm rate of zero or hit rate of one by .5/ntrials
tweak <- function(x,n){
idx <- which(x == 0.00)
x[idx] <- .5/n
idx <- which(is.nan(x))
x[idx] <- .5/n
idx <- which(x == 1.00)
x[idx] <- 1.00 - .5/n
return(x)
}
fa1 <- array(unlist(results_u$fa1))
results_u$fa1 <- as.numeric(tweak(fa1,ntrials))
results_u$dprime1 <- qnorm(results_u$h1) - qnorm(results_u$fa1) #usually convert hit rate and false alarm rate to zscores before subtracting
fa2 <- array(unlist(results_u$fa2))
results_u$fa2 <- as.numeric(tweak(fa2,ntrials))
results_u$dprime2 <- qnorm(results_u$h2) - qnorm(results_u$fa2)
fa3 <- array(unlist(results_u$fa3))
results_u$fa3 <- as.numeric(tweak(fa3,ntrials))
results_u$dprime3 <- qnorm(results_u$h3) - qnorm(results_u$fa3)
results_u <- results_u %>% rowwise() %>% mutate(dprime = mean(c(dprime1,dprime2,dprime3),na.rm=TRUE))
results_u <- untimed %>%
group_by(subject,phase,image_type,disambiguated) %>%
summarise(
n = n(),
accuracy = mean(correct),
rt = mean(rt),
h1 = sum(hit1)/(sum(miss1)+sum(hit1)), #when real image was present, how likely were they to respond 'real'?
fa1 = sum(false_alarm1)/(sum(absent1)+sum(false_alarm1)), #when real image was absent, how likely were they to respond 'real'?
h2 = sum(hit2)/(sum(miss2)+sum(hit2)),
fa2 = sum(false_alarm2)/(sum(absent2)+sum(false_alarm2)),
h3 = sum(hit3)/(sum(miss3)+sum(hit3)),
fa3 = sum(false_alarm3)/(sum(absent3)+sum(false_alarm3)),
confidence = mean(confidence)
)
ntrials <- results_u %>% pull(n)
ntrials <- ntrials[[1]]
results_u <- results_u %>% select(!n)
results_u$h1 <- as.numeric(tweak(array(unlist(results_u$h1)),ntrials))
results_u$fa1 <- as.numeric(tweak(array(unlist(results_u$fa1)),ntrials))
results_u$dprime1 <- qnorm(results_u$h1) - qnorm(results_u$fa1) #usually convert hit rate and false alarm rate to zscores before subtracting
results_u$h3 <- as.numeric(tweak(array(unlist(results_u$h3)),ntrials))
results_u$fa2 <- as.numeric(tweak(array(unlist(results_u$fa2)),ntrials))
results_u$dprime2 <- qnorm(results_u$h2) - qnorm(results_u$fa2)
results_u$h3 <- as.numeric(tweak(array(unlist(results_u$h3)),ntrials))
results_u$fa3 <- as.numeric(tweak(array(unlist(results_u$fa3)),ntrials))
results_u$dprime3 <- qnorm(results_u$h3) - qnorm(results_u$fa3)
results_u <- results_u %>% rowwise() %>% mutate(dprime = mean(c(dprime1,dprime2,dprime3),na.rm=TRUE))
### summarise
# summarise the self-timed task data
results_u <- untimed %>%
group_by(subject,phase,image_type,disambiguated) %>%
summarise(
n = n(),
accuracy = mean(correct),
rt = mean(rt),
h1 = sum(hit1)/(sum(miss1)+sum(hit1)), #when real image was present, how likely were they to respond 'real'?
fa1 = sum(false_alarm1)/(sum(absent1)+sum(false_alarm1)), #when real image was absent, how likely were they to respond 'real'?
h2 = sum(hit2)/(sum(miss2)+sum(hit2)),
fa2 = sum(false_alarm2)/(sum(absent2)+sum(false_alarm2)),
h3 = sum(hit3)/(sum(miss3)+sum(hit3)),
fa3 = sum(false_alarm3)/(sum(absent3)+sum(false_alarm3)),
confidence = mean(confidence)
)
ntrials <- results_u %>% pull(n)
ntrials <- ntrials[[1]]
results_u <- results_u %>% select(!n)
results_u$h1 <- as.numeric(tweak(array(unlist(results_u$h1)),ntrials))
results_u$fa1 <- as.numeric(tweak(array(unlist(results_u$fa1)),ntrials))
results_u$dprime1 <- qnorm(results_u$h1) - qnorm(results_u$fa1) #usually convert hit rate and false alarm rate to zscores before subtracting
results_u$h2 <- as.numeric(tweak(array(unlist(results_u$h2)),ntrials))
results_u$fa2 <- as.numeric(tweak(array(unlist(results_u$fa2)),ntrials))
results_u$dprime2 <- qnorm(results_u$h2) - qnorm(results_u$fa2)
results_u$h3 <- as.numeric(tweak(array(unlist(results_u$h3)),ntrials))
results_u$fa3 <- as.numeric(tweak(array(unlist(results_u$fa3)),ntrials))
results_u$dprime3 <- qnorm(results_u$h3) - qnorm(results_u$fa3)
results_u <- untimed %>%
group_by(subject,phase,image_type,disambiguated) %>%
summarise(
n = n(),
accuracy = mean(correct),
rt = mean(rt),
h1 = sum(hit1)/(sum(miss1)+sum(hit1)), #when real image was present, how likely were they to respond 'real'?
fa1 = sum(false_alarm1)/(sum(absent1)+sum(false_alarm1)), #when real image was absent, how likely were they to respond 'real'?
h2 = sum(hit2)/(sum(miss2)+sum(hit2)),
fa2 = sum(false_alarm2)/(sum(absent2)+sum(false_alarm2)),
h3 = sum(hit3)/(sum(miss3)+sum(hit3)),
fa3 = sum(false_alarm3)/(sum(absent3)+sum(false_alarm3)),
confidence = mean(confidence)
)
ntrials <- results_u %>% pull(n)
ntrials <- ntrials[[1]]
results_u <- results_u %>% select(!n)
test <- results_u %>% select(h1:fa3)
test <- results_u %>% ungroup() %>% select(h1:fa3)
View(test)
function(x){as.numeric(tweak(array(unlist(x)),ntrials))}
y <- function(x){as.numeric(tweak(array(unlist(x)),ntrials))}
test_out <- lapply(test,y)
View(test_out)
test_out <- sapply(test,y)
results_u <- untimed %>%
group_by(subject,phase,image_type,disambiguated) %>%
summarise(
n = n(),
accuracy = mean(correct),
rt = mean(rt),
h1 = sum(hit1)/(sum(miss1)+sum(hit1)), #when real image was present, how likely were they to respond 'real'?
fa1 = sum(false_alarm1)/(sum(absent1)+sum(false_alarm1)), #when real image was absent, how likely were they to respond 'real'?
h2 = sum(hit2)/(sum(miss2)+sum(hit2)),
fa2 = sum(false_alarm2)/(sum(absent2)+sum(false_alarm2)),
h3 = sum(hit3)/(sum(miss3)+sum(hit3)),
fa3 = sum(false_alarm3)/(sum(absent3)+sum(false_alarm3)),
confidence = mean(confidence)
)
ntrials <- results_u %>% pull(n)
ntrials <- ntrials[[1]]
results_u <- results_u %>% select(!n)
y <- function(x){as.numeric(tweak(array(unlist(x)),ntrials))}
hits_alarms <- results_u %>% ungroup() %>% select(h1:fa3)
results_u$h1:fa3 <- sapply(hits_alarms,y)
# summarise the self-timed task data
results_u <- untimed %>%
group_by(subject,phase,image_type,disambiguated) %>%
summarise(
n = n(),
accuracy = mean(correct),
rt = mean(rt),
h1 = sum(hit1)/(sum(miss1)+sum(hit1)), #when real image was present, how likely were they to respond 'real'?
fa1 = sum(false_alarm1)/(sum(absent1)+sum(false_alarm1)), #when real image was absent, how likely were they to respond 'real'?
h2 = sum(hit2)/(sum(miss2)+sum(hit2)),
fa2 = sum(false_alarm2)/(sum(absent2)+sum(false_alarm2)),
h3 = sum(hit3)/(sum(miss3)+sum(hit3)),
fa3 = sum(false_alarm3)/(sum(absent3)+sum(false_alarm3)),
confidence = mean(confidence)
)
ntrials <- results_u %>% pull(n)
ntrials <- ntrials[[1]]
results_u <- results_u %>% select(!n)
y <- function(x){as.numeric(tweak(array(unlist(x)),ntrials))}
hits_alarms <- results_u %>% ungroup() %>% select(h1:fa3)
hits_alarms <- sapply(hits_alarms,y)
results_u$h1:fa3 <- hits_alarms
results_u$h1:fa3 <- 1
results_u[,7:12] <- 1
results_u <- untimed %>%
group_by(subject,phase,image_type,disambiguated) %>%
summarise(
n = n(),
accuracy = mean(correct),
rt = mean(rt),
h1 = sum(hit1)/(sum(miss1)+sum(hit1)), #when real image was present, how likely were they to respond 'real'?
fa1 = sum(false_alarm1)/(sum(absent1)+sum(false_alarm1)), #when real image was absent, how likely were they to respond 'real'?
h2 = sum(hit2)/(sum(miss2)+sum(hit2)),
fa2 = sum(false_alarm2)/(sum(absent2)+sum(false_alarm2)),
h3 = sum(hit3)/(sum(miss3)+sum(hit3)),
fa3 = sum(false_alarm3)/(sum(absent3)+sum(false_alarm3)),
confidence = mean(confidence)
)
ntrials <- results_u %>% pull(n)
ntrials <- ntrials[[1]]
results_u <- results_u %>% select(!n)
y <- function(x){as.numeric(tweak(array(unlist(x)),ntrials))}
hits_alarms <- results_u %>% ungroup() %>% select(h1:fa3)
hits_alarms <- sapply(hits_alarms,y)
results_u[,7:12] <- hits_alarms
results_u[,h1:fa3] <- hits_alarms
results_u <- untimed %>%
group_by(subject,phase,image_type,disambiguated) %>%
summarise(
n = n(),
accuracy = mean(correct),
rt = mean(rt),
h1 = sum(hit1)/(sum(miss1)+sum(hit1)), #when real image was present, how likely were they to respond 'real'?
fa1 = sum(false_alarm1)/(sum(absent1)+sum(false_alarm1)), #when real image was absent, how likely were they to respond 'real'?
h2 = sum(hit2)/(sum(miss2)+sum(hit2)),
fa2 = sum(false_alarm2)/(sum(absent2)+sum(false_alarm2)),
h3 = sum(hit3)/(sum(miss3)+sum(hit3)),
fa3 = sum(false_alarm3)/(sum(absent3)+sum(false_alarm3)),
confidence = mean(confidence)
)
ntrials <- results_u %>% pull(n)
ntrials <- ntrials[[1]]
results_u <- results_u %>% select(!n)
y <- function(x){as.numeric(tweak(array(unlist(x)),ntrials))}
hits_alarms <- results_u %>% ungroup() %>% select(h1:fa3)
hits_alarms <- sapply(hits_alarms,y)
results_u[,7:12] <- hits_alarms
results_u$dprime1 <- qnorm(results_u$h1) - qnorm(results_u$fa1)
results_u$dprime2 <- qnorm(results_u$h2) - qnorm(results_u$fa2)
results_u$dprime3 <- qnorm(results_u$h3) - qnorm(results_u$fa3)
results_u <- results_u %>% rowwise() %>% mutate(dprime = mean(c(dprime1,dprime2,dprime3),na.rm=TRUE))
source(file.path(project_path,'src','format.R'))
untimed <- read.csv(fnl_u)
timed <- read.csv(fnl_t)
format(untimed,timed,project_path,date)
source(file.path(project_path,'src','format.R'))
untimed <- read.csv(fnl_u)
timed <- read.csv(fnl_t)
format(untimed,timed,project_path,date)
# lydia barnes, may 2024 counts, clusters, and/or traces stereotypical behaviour during training to
# explain performance at test
#--------------------------------------------------------------------------------------------------
# sources
library(tidyverse)
library(gtools)
source(file.path(getwd(), "src-stereo", "count_stereo.R"))
# settings
version <- "study-01" # pilot-data-00 (train and test), pilot-data-01 (learn and train), pilot-data-02 (learn and train, learn phase split into two parts)
exp <- "exp_lt" # experiment: 'exp_ts' (task-switching) or 'exp_lt' (learning transfer)
mes <- "clicks" # measure: 'clicks' or 'hovers'. usually want 'clicks'.
# paths
project_path <- getwd()
data_path <- file.path(project_path, "res")
if (!dir.exists(data_path)) {
stop(paste0(data_path, " does not exist"))
}
#--------------------------------------------------------------------------------------------------
# load event data
fnl <- file.path(data_path, paste(paste(version, exp, mes, "evt", sep = "_"), ".csv", sep = ""))
data <- read_csv(fnl, show_col_types = FALSE)
data <- data %>% filter(ses == 2)
# load shortest path data
fnl <- file.path(data_path, paste(paste(version, exp, mes, "opt-path", sep = "_"), ".csv", sep = ""))
opt <- read_csv(fnl, show_col_types = FALSE)
# load graph of distances between doors
fnl <- file.path(project_path, "src-stereo", "graph.csv")
graph <- unname(data.matrix(read_csv(fnl, col_names = FALSE, show_col_types = FALSE)))
utils::globalVariables(".data")
### insensitive to feedback?  count re-clicks on previous context doors on switch trials
events <- data %>%
filter(switch == 1, door_oc == 1)
reclicks <- events %>%
group_by(sub, ses, t, context, subses) %>%
summarise(n = n(), n_reclicks = n() - length(unique(door)))
reclicks <- reclicks %>%
group_by(sub, ses, context, subses) %>%
summarise(clicks = mean(n), reclicks = mean(n_reclicks))
### accurate?  count accuracy on stay trials
events <- data %>%
filter(switch == 0)
accuracy <- events %>%
group_by(sub, ses, t, context, subses) %>%
summarise(n_clicks = n(), n_correct = sum(door_cc), n_correct_oc = sum(door_oc),
accuracy = n_correct / n_clicks, metatask_accuracy = (n_correct+n_correct_oc)/n_clicks)
accuracy <- accuracy %>%
group_by(sub, ses, context, subses) %>%
summarise(accuracy = mean(accuracy), metatask_accuracy = mean(metatask_accuracy))
### consistent in transitions?
transitions <- data.frame(
sub = integer(), ses = integer(), context = integer(), subses = integer(),
transition_counts = double(), transition_weights = double()
)
su <- 1
se <- 2
co <- 1
ss <- 1
events <- data %>% filter(switch == 0, sub == su, ses == se, context == co, subses == ss, door_cc == 1)
transition_matrix <- matrix(0, nrow = 16, ncol = 16)
for (tr in unique(events$t)) {
trial <- events %>% filter(t == tr)
# if there's more than one event, record door transitions
if (nrow(trial) > 1) {
for (i in 2:nrow(trial)) {
door <- trial$door[i]
previous <- trial$door[i - 1]
transition_matrix[previous, door] <- transition_matrix[previous,door]+1
}
}
}
View(transition_matrix)
transition_counts <- colSums(as.matrix(transition_matrix > 0)+0)
transition_counts <- mean(transition_counts[transition_counts != 0])
transition_weights <- colSums(transition_matrix)/colMax(transition_matrix)
transition_weights
transition_weights <- mean(transition_weights[transition_weights != 0])
transition_weights
transition_weights <- colSums(transition_matrix)
transition_weights
colMax(transition_matrix)
colMax <- function(data) {
lapply(data, max, na.rm = TRUE, simplify = "array")
}
colMax(transition_matrix)
colMax <- function(data) {
lapply(data, max, na.rm = TRUE)
}
colMax(transition_matrix)
colMax <- function(data) {
sapply(data, max, na.rm = TRUE)
}
colMax(transition_matrix)
colMax(data.frame(transition_matrix))
data.matrix(colMax(data.frame(transition_matrix)))
data.matrix(colMax(data.frame(transition_matrix)))[1]
colMax(data.frame(transition_matrix))
colSums(transition_matrix)
colMax(data.frame(transition_matrix))/colSums(transition_matrix)
data.matrix(colMax(data.frame(transition_matrix)))/colSums(transition_matrix)
colMax(data.frame(transition_matrix))/colSums(transition_matrix)
mean(transition_weights[transition_weights != 0])
transition_weights <- colMax(data.frame(transition_matrix))/colSums(transition_matrix)
mean(transition_weights[transition_weights != 0])
transition_weights <- colMax(data.frame(transition_matrix))/colSums(transition_matrix)
transition_weights <- mean(transition_weights[!is.nan(transition_weights)])
transition_weights
sapply((colMax(data.frame(transition_matrix))/colSums(transition_matrix)),log)
(colMax(data.frame(transition_matrix))/colSums(transition_matrix))
log(.6)
(colMax(data.frame(transition_matrix))/colSums(transition_matrix)) + sapply((colMax(data.frame(transition_matrix))/colSums(transition_matrix)),log)
entropy <- mean(entropy[!is.nan(entropy)])
entropy <- (colMax(data.frame(transition_matrix))/colSums(transition_matrix)) + sapply((colMax(data.frame(transition_matrix))/colSums(transition_matrix)),log)
entropy <- mean(entropy[!is.nan(entropy)])
entropy
entropy <- (colMax(data.frame(transition_matrix))/colSums(transition_matrix)) + sapply((colMax(data.frame(transition_matrix))/colSums(transition_matrix)),log)
entropy <- 1-mean(entropy[!is.nan(entropy)])
entropy
entropy <- (colMax(data.frame(transition_matrix))/colSums(transition_matrix)) + sapply((colMax(data.frame(transition_matrix))/colSums(transition_matrix)),log)
entropy <- -mean(entropy[!is.nan(entropy)])
entropy
entropy <- (colMax(data.frame(transition_matrix))/colSums(transition_matrix)) + sapply((colMax(data.frame(transition_matrix))/colSums(transition_matrix)),log)
# sum the log probabilities and take the negative
entropy <- -sum(entropy[!is.nan(entropy)])
entropy
transition_matrix
(transition_matrix/colSums(transition_matrix))
sapply(data.frame(transition_matrix),function(x){x/sum(x)})
entropy <- entropy + sapply(entropy,log)
entropy
entropy <- sapply(data.frame(transition_matrix),function(x){x/sum(x)})
entropy
entropy + sapply(entropy,log)
entropy[is.infinite(entropy)] <- NaN
entropy
entropy <- entropy + sapply(entropy,log)
entropy[is.infinite(entropy)] <- NaN
entropy
entropy <- -sum(entropy[!is.nan(entropy)])
entropy
entropy <- sapply(data.frame(transition_matrix),function(x){x/sum(x)})
entropy <- entropy + sapply(entropy,log)
entropy[is.infinite(entropy)] <- NaN
entropy <- sapply(entropy,colSums) * -1
entropy <- sapply(data.frame(transition_matrix),function(x){x/sum(x)})
entropy <- entropy + sapply(entropy,log)
entropy[is.infinite(entropy)] <- NaN
entropy
entropy <- sapply(entropy,colSums)
entropy <- sapply(data.frame(transition_matrix),function(x){x/sum(x)})
entropy <- entropy + sapply(entropy,log)
entropy[is.infinite(entropy)] <- NaN
entropy <- colSums(entropy)
entropy
entropy <- sapply(data.frame(transition_matrix),function(x){x/sum(x)})
entropy <- entropy + sapply(entropy,log)
entropy[is.infinite(entropy)] <- NaN
entropy <- -colSums(entropy[!is.nan(entropy)])
entropy <- sapply(data.frame(transition_matrix),function(x){x/sum(x)})
entropy <- entropy + sapply(entropy,log)
entropy[is.infinite(entropy)] <- NaN
entropy <- colSums(entropy[!is.nan(entropy)])
entropy <- sapply(data.frame(transition_matrix),function(x){x/sum(x)})
entropy <- entropy + sapply(entropy,log)
entropy[is.infinite(entropy)] <- NaN
entropy
entropy[!is.nan(entropy)]
entropy <- colSums(entropy)
entropy
entropy <- sapply(data.frame(transition_matrix),function(x){x/sum(x)})
entropy <- entropy + sapply(entropy,log)
entropy[is.infinite(entropy)] <- NaN
entropy <- colSums(entropy,na.rm=TRUE)
entropy
entropy <- sapply(data.frame(transition_matrix),function(x){x/sum(x)})
entropy <- entropy + sapply(entropy,log)
entropy[is.infinite(entropy)] <- NaN
entropy <- -colSums(entropy,na.rm=TRUE)
entropy
transition_matrix
entropy <- mean(entropy[entropy != 0])
entropy
transition_weights <- colMax(data.frame(transition_matrix))/colSums(transition_matrix)
transition_weights
!is.nan(transition_weights)
transition_weights[!is.nan(transition_weights)]
source("~/Documents/academe/projects/doors/src-stereo/run_stereo.R", echo=TRUE)
View(stereo)
