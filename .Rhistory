# computer
data_path <- file.path("/Users/lydiabarnes/OneDrive - UNSW/task switch and transfer/data-sandpit", version)
if (!dir.exists(data_path)) {
stop(paste0(data_path, " does not exist"))
}
### load an up-to-date list of participants
subs <- get_subs(exp, version)
### extract events from the raw data
# make an empty data frame with all the variables (columns) that we will want
grp_data <- data.frame(
sub = integer(), ses = integer(), t = integer(), context = integer(), door = integer(),
door_cc = integer(), on = numeric(), off = numeric(), subses = integer(), door_oc = integer(),
switch = integer(), train_type = integer(), transfer = integer(), full_transfer_first = integer(),
original_house = integer()
)
# for each subject and session, use the function 'get_data' to load their raw data and attach it to
# our 'grp_data' data frame with one measurement (row) per event (click or hover)
for (sub in subs) {
print(sub)
sid <- as.numeric(substring(sub,5,7))
for (ses in sess) {
train_type <- NA
context_one_doors <- NA
if (ses == "ses-test") {
train_type <- grp_data %>%
filter(sub == sid, ses == 2) %>%
select(train_type) %>%
unique() %>%
pull()
train_doors <- grp_data %>%
filter(sub==sid,ses==ses,door_cc==1) %>%
select(door,context) %>%
unique()
}
data <- get_data(data_path, exp, sub, ses, train_type, train_doors) # load and format raw data
grp_data <- rbind(grp_data, data) # add to the 'grp_data' data frame so we end up with all subjects and sessions in one spreadsheet
}
}
# track whether context-incorrect clicks in the test phase land on doors that were learned in the train phase
if(exp=="exp_lt"){
door_lc <- get_learned_doors(grp_data)
grp_data$door_lc <- door_lc
}else{
grp_data <- grp_data %>% mutate(door_lc = c(kronecker(matrix(1, nrow(res), 1), NA)))
}
# save the formatted data
fnl <- file.path(project_path, "res", paste(paste(exp, "evt", sep = "_"), ".csv", sep = ""))
write_csv(grp_data, fnl)
### extract accuracy and response time averages from event data
# by trial
res <- grp_data %>%
group_by(sub, ses, t, context, train_type, transfer, full_transfer_first, original_house) %>%
summarise(
switch = max(switch), n_clicks = n(), n_cc = sum(door_cc), n_oc = sum(door_oc), n_lc = sum(door_lc),
accuracy = n_cc / n_clicks,
general_errors = (n_clicks - n_cc - n_oc) / n_clicks,
setting_errors = n_oc / n_clicks,
learned_setting_errors = n_lc / n_clicks
)
rt <- grp_data %>%
group_by(sub, ses, t, context, train_type, transfer) %>%
filter(door_cc == 1) %>%
summarise(rt = min(off)) # time to first correct click offset
res$rt <- rt$rt
# add accuracy calculated the way it was for points during the task
win <- 4-res$n_clicks >= 0
res$win <- win
# context changes
#   record the number of times they switch between door_cc, door_oc, and door_nc on each trial.
#   if it's a non-switch trial, assume that they're coming from door_cc. if it's a switch, assume that they're coming from door_oc.
grp_data <- grp_data %>% mutate(door_nc = case_when(door_cc==1 ~ 0, door_oc == 1 ~ 0, .default=1))
c(res$context_changes, res$learned_context_changes) %<-% get_context_changes(grp_data)
fnl <- file.path(project_path, "res", paste(paste(exp, "trl", sep = "_"), ".csv", sep = ""))
write_csv(res, fnl)
# by subject
res <- res %>%
group_by(sub, ses, context, switch, train_type, transfer, full_transfer_first, original_house) %>%
summarise_all(mean)
# transition probabilities
#   record how likely they are to move between the partial-transfer doors pairs during the train phase
res <- res %>% ungroup() %>% mutate(transition_probabilities = c(kronecker(matrix(1, nrow(res), 1), NA)))
if(exp=="exp_lt"){
res$transition_probabilities[which(res$ses==2)] <- get_transition_probabilities(grp_data)
}
fnl <- file.path(project_path, "res", paste(paste(exp, "avg", sep = "_"), ".csv", sep = ""))
write_csv(res, fnl)
# lydia barnes, march 2024 this script extracts, formats, and summarises data from the 'doors'
# project.
### sources
library(tidyverse)
library(zeallot) #unpack/destructure with %<-%
source(file.path(getwd(), "src", "get_subs.R"))
source(file.path(getwd(), "src", "get_switch.R"))
source(file.path(getwd(), "src", "get_data.R"))
source(file.path(getwd(), "src", "get_context_changes.R"))
source(file.path("src","get_transition_probabilities.R"))
source(file.path("src","get_learned_doors.R"))
### settings
# !you will want to update these settings a lot during piloting, when the task code or the way you
# test changes, or when you test participants on different subsets of the task phases
version <- "study-01" # pilot-data-00 (train and test), pilot-data-01 (learn and train), pilot-data-02 (learn and train, learn phase split into two parts)
exp <- "exp_lt" # experiment: 'exp_ts' (task-switching) or 'exp_lt' (learning transfer)
sess <- c("ses-learn","ses-train","ses-test") # session: 'ses-learn','ses-train','ses-test'. can select one (e.g. ses <- c('ses-learn')) or multiple (e.g. ses <- c('ses-train','ses-test'))
### paths
# !if you open the project thru doors.Rproj, your working directory will automatically be the
# project path
project_path <- getwd()
if (!dir.exists(file.path(project_path, "res"))) {
# check that the results directory exists. if it doesn't, create it.
dir.create(file.path(project_path, "res"))
}
# !you will need to change the data path to match the location of OneDrive on your personal
# computer
data_path <- file.path("/Users/lydiabarnes/OneDrive - UNSW/task switch and transfer/data-sandpit", version)
if (!dir.exists(data_path)) {
stop(paste0(data_path, " does not exist"))
}
### load an up-to-date list of participants
subs <- get_subs(exp, version)
### extract events from the raw data
# make an empty data frame with all the variables (columns) that we will want
grp_data <- data.frame(
sub = integer(), ses = integer(), t = integer(), context = integer(), door = integer(),
door_cc = integer(), on = numeric(), off = numeric(), subses = integer(), door_oc = integer(),
switch = integer(), train_type = integer(), transfer = integer(), full_transfer_first = integer(),
original_house = integer()
)
# for each subject and session, use the function 'get_data' to load their raw data and attach it to
# our 'grp_data' data frame with one measurement (row) per event (click or hover)
for (sub in subs) {
print(sub)
sid <- as.numeric(substring(sub,5,7))
for (ses in sess) {
train_type <- NA
context_one_doors <- NA
if (ses == "ses-test") {
train_type <- grp_data %>%
filter(sub == sid, ses == 2) %>%
select(train_type) %>%
unique() %>%
pull()
train_doors <- grp_data %>%
filter(sub==sid,ses==ses,door_cc==1) %>%
select(door,context) %>%
unique()
}
data <- get_data(data_path, exp, sub, ses, train_type, train_doors) # load and format raw data
grp_data <- rbind(grp_data, data) # add to the 'grp_data' data frame so we end up with all subjects and sessions in one spreadsheet
}
}
# track whether context-incorrect clicks in the test phase land on doors that were learned in the train phase
if(exp=="exp_lt"){
door_lc <- get_learned_doors(grp_data)
grp_data$door_lc <- door_lc
}else{
grp_data <- grp_data %>% mutate(door_lc = c(kronecker(matrix(1, nrow(res), 1), NA)))
}
# save the formatted data
fnl <- file.path(project_path, "res", paste(paste(exp, "evt", sep = "_"), ".csv", sep = ""))
write_csv(grp_data, fnl)
### extract accuracy and response time averages from event data
# by trial
res <- grp_data %>%
group_by(sub, ses, t, context, train_type, transfer, full_transfer_first, original_house) %>%
summarise(
switch = max(switch), n_clicks = n(), n_cc = sum(door_cc), n_oc = sum(door_oc), n_lc = sum(door_lc),
accuracy = n_cc / n_clicks,
general_errors = (n_clicks - n_cc - n_oc) / n_clicks,
setting_errors = n_oc / n_clicks,
learned_setting_errors = n_lc / n_clicks
)
rt <- grp_data %>%
group_by(sub, ses, t, context, train_type, transfer) %>%
filter(door_cc == 1) %>%
summarise(rt = min(off)) # time to first correct click offset
res$rt <- rt$rt
# add accuracy calculated the way it was for points during the task
win <- 4-res$n_clicks >= 0
res$win <- win
# context changes
#   record the number of times they switch between door_cc, door_oc, and door_nc on each trial.
#   if it's a non-switch trial, assume that they're coming from door_cc. if it's a switch, assume that they're coming from door_oc.
grp_data <- grp_data %>% mutate(door_nc = case_when(door_cc==1 ~ 0, door_oc == 1 ~ 0, .default=1))
c(res$context_changes, res$learned_context_changes) %<-% get_context_changes(grp_data)
fnl <- file.path(project_path, "res", paste(paste(exp, "trl", sep = "_"), ".csv", sep = ""))
write_csv(res, fnl)
# by subject
res <- res %>%
group_by(sub, ses, context, switch, train_type, transfer, full_transfer_first, original_house) %>%
summarise_all(mean)
# transition probabilities
#   record how likely they are to move between the partial-transfer doors pairs during the train phase
res <- res %>% ungroup() %>% mutate(transition_probabilities = c(kronecker(matrix(1, nrow(res), 1), NA)))
if(exp=="exp_lt"){
res$transition_probabilities[which(res$ses==2)] <- get_transition_probabilities(grp_data)
}
fnl <- file.path(project_path, "res", paste(paste(exp, "avg", sep = "_"), ".csv", sep = ""))
write_csv(res, fnl)
library(tidyverse)
library(ggforce)
library(ggsci)
# essentials
project_path <- getwd()
exp <- "exp_lt" # experiment: 'exp_ts' (task-conding) or 'exp_lt' (learning transfer)
ses <- "ses-test" # session: 'ses-learn','ses-train','ses-test'
label_sz <- 20
mk_sz <- 2
fnl <- file.path(project_path, "res", paste(paste(exp, "avg", sep = "_"), ".csv", sep = ""))
res <- read.csv(fnl)
res %>% filter(ses==2) %>% ggplot(aes(x=learned_context_changes,fill=factor(switch))) + geom_density()
res %>% filter(ses==2) %>% ggplot(aes(y=learned_context_changes,x=factor(switch))) + geom_violin()
res %>% filter(ses==2) %>% ggplot(aes(y=learned_context_changes,x=factor(switch),fill=factor(train_type))) + geom_violin()
res %>% filter(ses==2) %>% ggplot(aes(x=general_errors,fill=factor(switch))) + geom_density()
res %>% filter(ses==2) %>% ggplot(aes(y=general_errors,x=factor(switch),fill=factor(train_type))) + geom_violin()
# lydia barnes, august 2024
# visualises RT distributions per subject and condition
library(tidyverse)
library(ggforce)
library(ggsci)
library(ggpubr)
project_path <- getwd()
# settings
exp <- "exp_ts" # experiment: 'exp_ts' (task-conding) or 'exp_lt' (learning transfer)
ses <- "ses-test" # session: 'ses-learn','ses-train','ses-test'
label_sz <- 20
mk_sz <- 2
# data
fnl <- file.path(project_path, "res", paste(paste(exp, "trl", sep = "_"), ".csv", sep = ""))
res <- read.csv(fnl)
pl <- list()
#factor(ses), linetype =
for (subject in unique(res$sub)){
tmp <- res %>% filter(sub==subject,ses==3)
pl[[subject]] <- tmp %>%
ggplot() +
geom_density(aes(x=rt, colour = factor(switch)), linewidth = 1.5) +
#guides(linetype=FALSE) +
theme_minimal() +
scale_color_brewer(
name = "Switch",
#labels = c("Learn","Train","Test")
) +
labs(title = paste("Subject",subject,sep=" "), x = "RT", y = "Density") +
theme(
plot.title = element_text(size = label_sz),
axis.text.x = element_text(size = label_sz), axis.text.y = element_text(size = label_sz), legend.text = element_text(size = label_sz),
axis.title.x = element_text(size = label_sz), axis.title.y = element_text(size = label_sz), legend.title = element_text(size = label_sz)
)
}
ggarrange(plotlist=pl,nrow=25,ncol=4)
fnl <- file.path(project_path, "fig", paste(paste(exp, "rt-distributions", sep = "_"), ".png", sep = ""))
ggsave(fnl, plot = last_plot(), width = 18, height = 90, limitsize = FALSE)
fnl <- file.path(project_path, "res", paste(paste(exp, "avg", sep = "_"), ".csv", sep = ""))
res <- read.csv(fnl)
res %>% filter(ses==2) %>% ggplot(aes(x=transition_probabilities,fill=factor(train_type)) + geom_density()
res %>% filter(ses==2) %>% ggplot(aes(x=transition_probabilities,fill=factor(train_type))) + geom_density()
View(res)
exp <- "exp_lt" # experiment: 'exp_ts' (task-conding) or 'exp_lt' (learning transfer)
### plot the data!
fnl <- file.path(project_path, "res", paste(paste(exp, "avg", sep = "_"), ".csv", sep = ""))
res <- read.csv(fnl)
res %>% filter(ses==2) %>% ggplot(aes(x=transition_probabilities,fill=factor(train_type))) + geom_density()
res %>% filter(ses==2) %>% ggplot(aes(x=transition_probabilities,fill=factor(train_type))) + geom_density(alpha=.5)
res %>% filter(ses==2) %>% group_by(sub,ses,switch) %>% summarise(mu=mean(transition_probabilities)) %>% ggplot(aes(x=transition_probabilities,fill=factor(train_type))) + geom_density(alpha=.5)
res %>% filter(ses==2) %>% group_by(sub,ses,switch) %>% summarise(mu=mean(transition_probabilities)) %>% ggplot(aes(x=mu,fill=factor(train_type))) + geom_density(alpha=.5)
res %>% filter(ses==2) %>% group_by(sub,ses,switch,train_type) %>% summarise(mu=mean(transition_probabilities)) %>% ggplot(aes(x=mu,fill=factor(train_type))) + geom_density(alpha=.5)
View(res)
res %>% filter(ses==2) %>% group_by(sub,ses,switch,train_type) %>% summarise(mu=mean(transition_probabilities)) %>% ggplot(aes(y=accuracy,x=mu,fill=factor(train_type))) + geom_point(alpha=.5)
res %>% filter(ses==2) %>% group_by(sub,ses,switch,train_type) %>% summarise_all(mean) %>% ggplot(aes(y=accuracy,x=transition_probabilities,fill=factor(train_type))) + geom_point(alpha=.5)
res %>% filter(ses==2) %>% group_by(sub,ses,switch,train_type) %>% summarise_all(mean) %>% ggplot(aes(y=accuracy,x=transition_probabilities) + geom_point(alpha=.5)
res %>% filter(ses==3,transfer==1) %>% group_by(sub,ses,switch,train_type) %>% summarise_all(mean) %>% ggplot(aes(y=accuracy,x=transition_probabilities)) + geom_point(alpha=.5)
res %>% filter(ses==3,transfer==1) %>% group_by(sub,ses,switch,train_type) %>% summarise_all(mean) %>% ggplot(aes(y=accuracy,x=transition_probabilities)) + geom_point(alpha=.5)
View(res)
res %>% filter(ses==3,transfer==1) %>% ggplot(aes(y=accuracy,x=transition_probabilities)) + geom_point(alpha=.5)
data <- res %>% filter(ses==3,transfer==1)
min(data$accuracy)
max(data$accuracy)
max(data$transition_probabilities)
min(data$transition_probabilities)
View(res)
res %>% filter(ses == 2) %>% select(sub, transition_probabilities) %>% group_by(sub) %>% summarise(mu = mean(transition_probabilities))
res %>% filter(ses == 2, switch==0) %>% select(sub, transition_probabilities) %>% group_by(sub) %>% summarise(mu = mean(transition_probabilities))
inner_join(res %>% filter(ses == 2, switch==0) %>% select(sub, transition_probabilities) %>% group_by(sub) %>% summarise(mu = mean(transition_probabilities)), res %>% filter(ses==3,transfer==2,switch==0) %>% select(sub, accuracy), by="sub")
transfer_data <- inner_join(res %>% filter(ses == 2, switch==0) %>% select(sub, transition_probabilities) %>% group_by(sub) %>% summarise(mu = mean(transition_probabilities)), res %>% filter(ses==3,transfer==2,switch==0) %>% select(sub, accuracy), by="sub")
transfer_data %>% ggplot(aes(x=mu,y=accuracy)) + geom_point()
transfer_data <- inner_join(res %>% filter(ses == 2, switch==0) %>% select(sub, transition_probabilities) %>% group_by(sub) %>% summarise(mu = mean(transition_probabilities)), res %>% filter(ses==3,transfer==2,switch==0) %>% select(sub, learned_setting_errors), by="sub")
transfer_data %>% ggplot(aes(x=mu,y=learned_setting_errors)) + geom_point()
transfer_data <- inner_join(res %>% filter(ses == 2, switch==0) %>% select(sub, transition_probabilities) %>% group_by(sub) %>% summarise(mu = mean(transition_probabilities)), res %>% filter(ses==3,transfer==2,switch==0) %>% select(sub, learned_setting_errors,full_transfer_first), by="sub")
View(transfer_data)
transfer_data %>% ggplot(aes(x=mu,y=learned_setting_errors,fill=factor(full_transfer_first))) + geom_point()
transfer_data %>% ggplot(aes(x=mu,y=learned_setting_errors,colour=factor(full_transfer_first))) + geom_point()
transfer_data %>% ggplot(aes(x=learned_setting_errors,fill=factor(full_transfer_first))) + geom_density(alpha=.7)
## K. Garner, 2024
### simulate data from a Dirichlet distribution and recover the parameters,
## plus compute entropy
## this code is meant to serve as a basis for a script to run over participants,
## as the coding great Lydia Barnes sees fit
rm(list=ls())
###### some helpful things
library(withr)
library(tidyverse)
library(rstan) # installation guide here - https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started
library(brms) # use regular install.packages()
library(tidybayes)
library(wesanderson)
n_routines = 6 # there are 6 possible routines
alpha_k <- rep(n_routines, times=n_routines) # shape parameter, like the beta,
# here our prior says that each route is likely to happen 1/6 of the time
# aka I am generating data under the null hypothesis
n_observations <- 160 # assuming the total counts is 160
# distribution counts, but for each possible routine
# each trial is a random draw from
sim_data <- with_seed(42, {
round(apply(rdirichlet(n=1e5, alpha=alpha_k),2,mean)*n_observations, 0) |>
data.frame() |>
pivot_longer(everything(), names_to = "r", values_to="counts")
})
sim_data$r <- paste("r", 1:n_routines, sep="")
View(sim_data)
############################################
## step 2
# first thing I want to do is compute the alphas for the
# dirichlet distribution (i.e. add the prior to all the
# counts), and visualise the results as a beta
# distribution for each alpha parameter - i.e. the probability of each
# routine being selected
dir_prior <- 1 # uniform prior (note this is the prior we'll use
# for modelling the data)
sim_data <- sim_data %>% mutate(posterior_alphas = counts + dir_prior)
View(sim_data)
# now I have the posterior alphas, I can simulate the distribution
# and visualise the results
with_seed(42, {
rdirichlet(n = 1e5, alpha = sim_data$posterior_alphas) |>
data.frame() |>
set_names(paste("α", 1:6, sep="")) |>
pivot_longer(everything(), values_to="p", names_to="alpha") |>
ggplot(aes(x = p, fill = alpha)) +
geom_density(bounds = c(0, 1), color = NA) +
scale_fill_manual(values = wes_palette("IsleofDogs1"), guide = "none") +
labs(x = "P") +
facet_wrap(vars(alpha))
})
## inference #1: a bayes factor, the strength of which tells
## us how likely the null probabilities are, relative to the observed
## probabilities for that participant
## the probability of the alternate hypothesis (that the expected values
# are larger than 1/6, relative to the probability
# that all thetas are likely to be around 1/6
# note that a larger value is greater evidence for the alternate hypothesis
null_hyp <- rep(1/n_routines, times=n_routines)
null_hyp
null_llike <- ddirichlet(null_hyp, alpha=sim_data$posterior_alphas, log=TRUE)
null_llike
n_routines = 6 # there are 6 possible routines
alpha_k <- rep(n_routines, times=n_routines) # shape parameter, like the beta,
alpha_k <- c(10, 1, 1, 1, 1, 1)
# here our prior says that each route is likely to happen 1/6 of the time
# aka I am generating data under the null hypothesis
n_observations <- 160 # assuming the total counts is 160
# distribution counts, but for each possible routine
# each trial is a random draw from
sim_data <- with_seed(42, {
round(apply(rdirichlet(n=1e5, alpha=alpha_k),2,mean)*n_observations, 0) |>
data.frame() |>
pivot_longer(everything(), names_to = "r", values_to="counts")
})
sim_data$r <- paste("r", 1:n_routines, sep="")
############################################
## step 2
# first thing I want to do is compute the alphas for the
# dirichlet distribution (i.e. add the prior to all the
# counts), and visualise the results as a beta
# distribution for each alpha parameter - i.e. the probability of each
# routine being selected
dir_prior <- 1 # uniform prior (note this is the prior we'll use
# for modelling the data)
sim_data <- sim_data %>% mutate(posterior_alphas = counts + dir_prior)
# now I have the posterior alphas, I can simulate the distribution
# and visualise the results
with_seed(42, {
rdirichlet(n = 1e5, alpha = sim_data$posterior_alphas) |>
data.frame() |>
set_names(paste("α", 1:6, sep="")) |>
pivot_longer(everything(), values_to="p", names_to="alpha") |>
ggplot(aes(x = p, fill = alpha)) +
geom_density(bounds = c(0, 1), color = NA) +
scale_fill_manual(values = wes_palette("IsleofDogs1"), guide = "none") +
labs(x = "P") +
facet_wrap(vars(alpha))
})
## inference #1: a bayes factor, the strength of which tells
## us how likely the null probabilities are, relative to the observed
## probabilities for that participant
## the probability of the alternate hypothesis (that the expected values
# are larger than 1/6, relative to the probability
# that all thetas are likely to be around 1/6
# note that a larger value is greater evidence for the alternate hypothesis
null_hyp <- rep(1/n_routines, times=n_routines)
null_llike <- ddirichlet(null_hyp, alpha=sim_data$posterior_alphas, log=TRUE)
null_llike
exp_theta <- sim_data$posterior_alphas/sum(sim_data$posterior_alphas)
exp_theta
exp_llike <- ddirichlet(exp_theta, alpha=sim_data$posterior_alphas, log=TRUE)
exp_llike
lBF <- exp_llike - null_llike # this is the BF in log p form
BF <- exp(lBF) # this is our bayes factor
BF
hyp_dat <- with_seed(42, {
rdirichlet(n = 1e5, alpha = sim_data$posterior_alphas) |>
data.frame() |>
set_names(paste("α", 1:6, sep="")) |>
pivot_longer(everything(), values_to="p", names_to="alpha") |>
group_by(alpha) |>
summarise(prob = mean(p <= 1/n_routines))
})
hyp_dat
dir_entropy <- function(alphas){
# see definition at https://en.wikipedia.org/wiki/Dirichlet_distribution
alpha0 <- sum(alphas)
K <- length(alphas)
log_beta_alpha <- sum(lgamma(alphas)) - lgamma(alpha0)
log_beta_alpha + (alpha0 - K)*digamma(alpha0) - sum((alphas-1)*digamma(alphas))
}
with(sim_data, dir_entropy(posterior_alphas))
### -----------------------------------------------------------------------
# resources
# McElreath (2020). Rethinking Statistics. Chap 8: God Spiked the Integers
rm(list=ls())
### -----------------------------------------------------------------------
# helpful things
library(withr)
library(tidyverse)
library(rstan)
library(brms)
### -----------------------------------------------------------------------
# 1. generate some dummy data
# first I'll load our predictors, which I saved from the
# 'bayes-compute-context-probs' doc
load("src-optimal/posterior-odds.Rda")
# first, its not sensible to think about 4 separate null selections
# from n, as the fourth has to hit the target, so removing that from the
# dataframe, also removing instances when m = 4, as the actor would have
# certainty about context at that point
odds <- odds %>% filter(n != 4) %>% filter(m != 4)
# now I am going to make this dataframe wider, to that I can more easily
# generate some fake data
odds <- odds %>% pivot_wider(names_from = condition, values_from = odds)
# note that because 'success|context' is a single constant,
# this is akin to saying that the data can be modelled using only a grand mean
# and a subject intercept, so going to drop that from the
# dataframe
odds[,"success|context"] <- NULL
# now I am going to assume 100 subjects, and simulate the selection of m door
# contexts, given a state of n and m
# I am going to assume a main effect for the context regressor, a subject intercept
# and random error
generate_count_dat_one_sub <- function(odds, i){
# define main effects
a <- -0.2 # intercept
b_c <- 0.25 # beta context
# other useful things
total_states <- nrow(odds)
n_trials <- 160 # we have 160 trials, and 16 states, so I am going
context <- odds$context
# to assume the total visits to each state is the EV, give or take a
# little bit
total_times_in_each_state <- round(rnorm(total_states,
mean=n_trials/total_states,
sd=1))
# calculate the p for each state
log_odds <- a + b_c*context + rnorm(1, mean=0, sd=2) + rnorm(length(context)) # note
# exactly how you generate the log odds will depend on the model you are testing
# e.g. I changed it when testing the hierarchal model
p <- 1/(1+exp(-log_odds))
# for each p, draw from the binomal distribution
resp <- unlist(lapply(p, rbinom, n=total_times_in_each_state, size=1))
context <- rep(context, each=length(p))
data <- data.frame(sub = i,
context = context,
resp = resp)
data
}
# now I'll generate data for 100 subs
dummy_data <- do.call(rbind, lapply(1:100, generate_count_dat_one_sub, odds=odds))
# to scale or not to scale context? good discussion here
# https://discourse.mc-stan.org/t/re-scaling-data-and-parameters-to-0-1/16275/4
# leaving for now
#dummy_data$test <- scale(dummy_data$context)
### -----------------------------------------------------------------------
# 2. setting up the model
# first lets set up a model that contains only the subject intercept and the
# grand intercept
source('src-optimal/stan-binomial-model-grandintercept-only.R') # load the model string
# step 1 is to make the variables that will be fed into the stan model
# note I am making more variables than strictly needed, thats for the next stage
ndata <- nrow(dummy_data) # n of observations
nsubs <- length(unique(dummy_data$sub))
context <- dummy_data$context
sub <- dummy_data$sub
resp <- dummy_data$resp
# make a nice data list for stan
data_list <- list(ndata = ndata, context = context,
resp=resp, nsubs = nsubs, sub = sub)
### -----------------------------------------------------------------------
# 3. Running the model
# the short, hopefully simple bit
int_model <- stan(model_code = get("int_only_model"),
data = data_list,
warmup = 1000,
iter = 2000,
chains = 4)
