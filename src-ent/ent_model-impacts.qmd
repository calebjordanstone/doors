---
title: "ent_model-impacts"
format: html
editor: visual
---

## Modelling the impact of the training manipulation on routine formation

```{r}

library(tidyverse)
id = 'kelly'
source(paste('/home', id, 'Documents/projects/doors/src-ent/ent_functions.R', sep='/'))
```

We know that the training manipulation impacts our measure of routines. The next question is what the two groups learned over the course of the routine.

## Hyotheses

We seek to test the following hypotheses:

1.  The rapid switch group mixed up the tasks early on and they stayed mixed up. This assumes the probability of other context (oc) errors is uniform across the session.
2.  All people were more likely to mix up the tasks the more switches they experienced. This assumes that the probability of oc errors follows a step function, with p(oc) increasing with each time the target found belongs to the context that is **not** the context where the last target was found.
3.  People learned the switch rate - i.e. the probability of oc follows n_tgt_switches / n_trials.
4.  People learned to track the probability of a switch, given the number of targets already found in the current context. i.e. how many times before did I do X number of trials and not switch?
5.  People track the probability of the context, given the number of non-target doors they have selected from the current context, since selection of the last target

Note that we can ask whether these factors influenced the probability of oc responses, and the probability of nc responses, to see if the proposed process influenced the mixing of task sets, or general confusion.

## Defining the model to test the four hypotheses

For each subject, we can model the probability of a given response being from the oc (or nc) class, using the following logistic regression:

$$
\mathrm{OC_1} \sim B(n, p_i) \\
\mathrm{log}(\frac{p_i}{1-p_i}) = \beta_0 + \beta \mathrm{Sw} + \beta \mathrm{TgtN} + \beta t_i + \beta \mathrm{N_{0|c}} + \epsilon
$$

where Sw is the number of switches experienced to this point, Tgt is the number of switches that have occurred after finding that many targets (in the past), t is the number of trials, and N_0\|c is the number of non-target doors drawn from the last rewarded context on that trial.

**An oc response is defined as the number of selections from the context that you were not last rewarded from.**

### Demonstrating the regressors track and behave in line with the hypotheses we seek to test

Here I load in data from one participant, compute the regressors, and plot them to show they predict what we think.

```{r}

# get data, and select what needed
dat <- read.csv(paste('/home', id, 'Documents/projects/doors-data/data-wrangled/exp_lt_evt.csv', sep='/')) %>% 
  filter(sub == 3 & ses == 2) %>% 
  select(sub, t, context, door, door_cc, door_oc, door_nc, switch)


```

First we build the Sw regressor

```{r}

get_Sw <- function(dat, subN){
  # this regrssor computes the number of times a target has been found in a context that is different to where the preceeding context was found, and creates a regressor as long as the responses that reflects this
  # the logical conditions for this are:
  # 1. is this a switch trial? yes/no - is 'switch' a 1?
  # 2. when is the end of the switch trial (this means the target has been found)
  #     for 2: I can get the trial numbers of each switch trial, and then find the 
  #           last entry in the data frame that corresponds to that trial number. 
  # 3. from sw_end + 1, the count goes up
  #     The row after the last entry above is sw+1
  
  tmp <- dat %>% filter(sub == subN)
  sw <- rep(0, nrow(tmp))
  sw_trls <- unique(tmp$t[as.logical(tmp$switch)]) # this gives me the trial numbers where a switch occurred
  sw_tgts_idx <- unlist(lapply(sw_trls, function(x) tail(which(tmp$t == x), 1))) 
  cumsum = 0:length(sw_tgts_idx)
  # add final row of dataframe to idx
  sw_tgts_idx <- c(sw_tgts_idx, nrow(tmp))
  rep_idx <- c(sw_tgts_idx[1], diff(sw_tgts_idx)) # now get number of times each switch should be repeated, before the next switch
  sw <- rep(cumsum, times = rep_idx)
  tmp$Sw <- sw
  tmp
}

dat <- get_Sw(dat, 3)
dat %>% group_by(t) %>% summarise(sw = Sw[1]) %>% ggplot(aes(x=t, y=sw)) + geom_line()
```

We already have the trial number vector, so this combined with Sw will give us switch rate. Specifically:

```{r}

dat %>% group_by(t) %>% summarise(sw = Sw[1]) %>% mutate(sr = sw/t) %>% 
  ggplot(aes(x=t, y=sr)) + geom_line()

```

The next regressor is: How many times before did I select X number of targets and not experience a switch?

I will initialise this regressor with a 1, to reflect the idea that people know a switch could happen any time.

Whenever I hit a target from a switch trial, I look back and count how many targets (N) there were in the other context before I hit this switch. I add a 1 to the tally for the 1:N elements.

I then label the next trial forwards with the number from my ongoing tally.

```{r}

get_TgtN <- function(dat, subN){
  # in this regressor, we learn the number of times a target was found in the other context, when the previous N targets were from the other context.
  # For this, we need
  # 1. to find every non-switch trial
  # 2. number each trial for how far it is from the previous switch trial. This is how many targets have been found in the current context.
  # 3. do a culmulative sum of how many times each number occurs
  # 4. create a vector that increments the culmulative sum
  
  # get subject data
  tmp <- dat %>% filter(sub == subN)
  
  swch_idx <- unique(tmp$t[dat$switch == 1])
  n_til_swch <- c(swch_idx[1]-1, diff(swch_idx)) # this gives us, from the first trial onwards, how many targets were found in the current context, up to the switch -aka a target being found in the other context. 
  # now, I want to culmulatively learn how many of each number have occurred at each point in the sequence
  # First, I make a vector that has as many elements 1:max(in n_til_swch)
  counts <- rep(0, max(n_til_swch))
  names(counts) = paste(1:max(n_til_swch))
  # next, I want to know, for every element in n_til_swch, how many times has 1:each possible element occurred up until that point?
  tgtNs <- list()
  
  for (i in 1:length(n_til_swch)){
    thisN = n_til_swch[i] # how many trials between the last two switches?
    update_idxs <- paste(1:thisN) 
    counts[update_idxs] <- (counts[update_idxs] + 1)
    ps = counts/sum(counts) # convert to probabililty
    tgtNs[[i]] <- ps
  }
  # now I have the estimate of the hazard function, computed once the target is found on every switch trial.
  # the next step is to from each swch trial to the next one, fill out the number of trials with the estimates of the hazard function. For example, if the first switch was on trial 2, and the next was on trial 5, then I apply the first 3 elements of the first estimate to trials 3, 4, & 5
  swch_idx <- swch_idx # where each switch starts
  end_idx <- c(swch_idx[2:length(swch_idx)], max(tmp$t))
  swch_idx <- swch_idx + 1 # to reflect updating after switches
  names(tgtNs) <- swch_idx
  
  # now I have the indexs, I can do the updating
  get_estimates_for_trials <- function(strti, endi, tgtNs){
    t <- strti:endi
    ests <- tgtNs[[paste(strti)]][1:length(strti:endi)]
    tibble(t = t,
          TgtN = ests)
  }
  ests <- do.call(rbind, mapply(get_estimates_for_trials, swch_idx, end_idx,
                        MoreArgs = list(tgtNs), SIMPLIFY = FALSE))
  # add the first trials to the end of the tibble
  ests <- rbind(ests, tibble(t=1:(swch_idx[1]-1),
                     TgtN = rep(0, length(1:(swch_idx[1]-1)))))
  
  tmp <- inner_join(tmp, ests, by="t")
  tmp
}

dat <- get_TgtN(dat, 3)

dat %>% group_by(t) %>% summarise(tgtN = TgtN[1]) %>% ggplot(aes(x=t, y=tgtN)) + geom_line()

```

```{r}

with(dat, cor.test(Sw, TgtN))
```

**Last one**: People track the probability of the context, given the number of non-target doors they have selected from the current context, since selection of the last target.
