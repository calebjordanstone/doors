---
title: "ent_model-impacts"
format: html
editor: visual
---

## Modelling the impact of the training manipulation on routine formation

```{r}

library(tidyverse)
library(GGally)
library(gridExtra)

id = 'kgarner'
source(paste('/home', id, 'Documents/projects/doors/src-ent/ent_functions.R', sep='/'))
```

We know that the training manipulation impacts our measure of routines. The next question is what the two groups learned over the course of the routine.

## Hyotheses

We seek to test the following hypotheses:

1.  The rapid switch group mixed up the tasks early on and they stayed mixed up. This assumes the probability of other context (oc) errors is uniform across the session.
2.  All people were more likely to mix up the tasks the more switches they experienced. This assumes that the probability of oc errors follows a step function, with p(oc) increasing with each time the target found belongs to the context that is **not** the context where the last target was found.
3.  People learned the switch rate - i.e. the probability of oc follows n_tgt_switches / n_trials.
4.  People learned to track the probability of a switch, given the number of targets already found in the current context. i.e. how many times before did I do X number of trials and not switch?
5.  People track the probability of the context, given the number of non-target doors they have selected from the current context, since selection of the last target

Note that we can ask whether these factors influenced the probability of oc responses, and the probability of nc responses, to see if the proposed process influenced the mixing of task sets, or general confusion.

## Defining the model to test the four hypotheses

For each subject, we can model the probability of a given response being from the oc (or nc) class, using the following logistic regression:

$$
\mathrm{OC_1} \sim B(n, p_i) \\
\mathrm{log}(\frac{p_i}{1-p_i}) = \beta_0 + \beta \mathrm{Sw} + \beta \mathrm{TgtN} + \beta t_i + \beta \mathrm{N_{0|c}} + \epsilon
$$

where Sw is the number of switches experienced to this point, Tgt is the number of switches that have occurred after finding that many targets (in the past), t is the number of trials, and N_0\|c is the number of non-target doors drawn from the last rewarded context on that trial.

**An oc response is defined as the number of selections from the context that you were not last rewarded from.**

### Demonstrating the regressors track and behave in line with the hypotheses we seek to test

Here I load in data from one participant, compute the regressors, and plot them to show they predict what we think.

```{r}

# get data, and select what needed
subN = 4
dat <- read.csv(paste('/home', id, 'Documents/projects/doors-data/data-wrangled/exp_lt_evt.csv', sep='/')) %>% 
  filter(sub == subN & ses == 2) %>% 
  select(sub, t, context, door, door_cc, door_oc, door_nc, switch)


```

First we build the Sw regressor

```{r}

get_Sw <- function(dat, subN){
  # this regrssor computes the number of times a target has been found in a context that is different to where the preceeding context was found, and creates a regressor as long as the responses that reflects this
  # the logical conditions for this are:
  # 1. is this a switch trial? yes/no - is 'switch' a 1?
  # 2. when is the end of the switch trial (this means the target has been found)
  #     for 2: I can get the trial numbers of each switch trial, and then find the 
  #           last entry in the data frame that corresponds to that trial number. 
  # 3. from sw_end + 1, the count goes up
  #     The row after the last entry above is sw+1
  
  tmp <- dat %>% filter(sub == subN)
  sw <- rep(0, nrow(tmp))
  sw_trls <- unique(tmp$t[as.logical(tmp$switch)]) # this gives me the trial numbers where a switch occurred
  sw_tgts_idx <- unlist(lapply(sw_trls, function(x) tail(which(tmp$t == x), 1))) 
  cumsum = 0:length(sw_tgts_idx)
  # add final row of dataframe to idx
  sw_tgts_idx <- c(sw_tgts_idx, nrow(tmp))
  rep_idx <- c(sw_tgts_idx[1], diff(sw_tgts_idx)) # now get number of times each switch should be repeated, before the next switch
  sw <- rep(cumsum, times = rep_idx)
  tmp$Sw <- sw
  tmp
}

dat <- get_Sw(dat, subN = subN)
Sw_p <- dat %>% group_by(t) %>% summarise(sw = Sw[1]) %>% ggplot(aes(x=t, y=sw)) + geom_line()
```

We already have the trial number vector, so this combined with Sw will give us switch rate. Specifically:

```{r}

Sw_r_p <- dat %>% group_by(t) %>% summarise(sw = Sw[1]) %>% mutate(sr = sw/t) %>% 
  ggplot(aes(x=t, y=sr)) + geom_line()

```

The next regressor is: How many times before did I select X number of targets and not experience a switch?

I will initialise this regressor with a 1, to reflect the idea that people know a switch could happen any time.

Whenever I hit a target from a switch trial, I look back and count how many targets (N) there were in the other context before I hit this switch. I add a 1 to the tally for the 1:N elements.

I then label the next trial forwards with the number from my ongoing tally.

```{r}

get_TgtN <- function(dat, subN){
  # in this regressor, we learn the number of times a target was found in the other context, when the previous N targets were from the other context.
  # For this, we need
  # 1. to find every non-switch trial
  # 2. number each trial for how far it is from the previous switch trial. This is how many targets have been found in the current context.
  # 3. do a culmulative sum of how many times each number occurs
  # 4. create a vector that increments the culmulative sum
  
  # get subject data
  tmp <- dat %>% filter(sub == subN)
  
  swch_idx <- unique(tmp$t[dat$switch == 1])
  n_til_swch <- c(swch_idx[1]-1, diff(swch_idx)) # this gives us, from the first trial onwards, how many targets were found in the current context, up to the switch -aka a target being found in the other context. 
  # now, I want to culmulatively learn how many of each number have occurred at each point in the sequence
  # First, I make a vector that has as many elements 1:max(in n_til_swch)
  counts <- rep(0, max(n_til_swch))
  names(counts) = paste(1:max(n_til_swch))
  # next, I want to know, for every element in n_til_swch, how many times has 1:each possible element occurred up until that point?
  tgtNs <- list()
  
  for (i in 1:length(n_til_swch)){
    thisN = n_til_swch[i] # how many trials between the last two switches?
    update_idxs <- paste(1:thisN) 
    counts[update_idxs] <- (counts[update_idxs] + 1)
    ps = counts/sum(counts) # convert to probabililty
    tgtNs[[i]] <- ps
  }
  # now I have the estimate of the hazard function, computed once the target is found on every switch trial.
  # the next step is to from each swch trial to the next one, fill out the number of trials with the estimates of the hazard function. For example, if the first switch was on trial 2, and the next was on trial 5, then I apply the first 3 elements of the first estimate to trials 3, 4, & 5
  swch_idx <- swch_idx # where each switch starts
  end_idx <- c(swch_idx[2:length(swch_idx)], max(tmp$t))
  swch_idx <- swch_idx + 1 # to reflect updating after switches
  names(tgtNs) <- swch_idx
  
  # now I have the indexs, I can do the updating
  get_estimates_for_trials <- function(strti, endi, tgtNs){
    t <- strti:endi
    ests <- tgtNs[[paste(strti)]][1:length(strti:endi)]
    tibble(t = t,
          TgtN = ests)
  }
  ests <- do.call(rbind, mapply(get_estimates_for_trials, swch_idx, end_idx,
                        MoreArgs = list(tgtNs), SIMPLIFY = FALSE))
  # add the first trials to the end of the tibble
  ests <- rbind(ests, tibble(t=1:(swch_idx[1]-1),
                     TgtN = rep(0, length(1:(swch_idx[1]-1)))))
  
  tmp <- inner_join(tmp, ests, by="t")
  tmp
}

dat <- get_TgtN(dat, subN = subN)

TgtN_p <- dat %>% group_by(t) %>% summarise(tgtN = TgtN[1]) %>% ggplot(aes(x=t, y=tgtN)) + geom_line()

```

**Last one**: People track the probability of the context, given the number of non-target doors they have selected from the current context, since selection of the last target.

Using Bayes theorem, we can cast the probability that you are still in $C_A$, given you have observed n and m false doors:

$$P(C = A | n, m) = \frac{ P(n,m|C=A)P(C=A)}{P(n,m)}$$

Here is the formula in a function (from other notes). Tis here for reference/as a reminder:

```{r}

get_prob_context <- function(priors, ps_g_c, n, m){
  # n is a string, matching the name of the number of n drawn
  # note that m doesn't require definition for the first prior (nor n for the second), as p(m) given the other context is always 1, thus any number of m is implicit in the calculation
  ps_g_c[n]*priors[1]/(ps_g_c[n]*priors[1] + ps_g_c[m]*priors[2])
}
```

For this, I will need the following columns of information:

1.  p(context A) given experience
2.  For each trial, the ongoing number of fails from the context for which you were last rewarded
3.  For each trial, the ongoing number of fails from the context for which you were not last rewarded

```{r}

get_p_context <- function(dat, subN){
  # for each trial, get the probability of being in the same context as the last rewarded trial
  
  tmp <- dat %>% filter(sub == subN)
  ##############################################
  # apply an update for each trial that reflects the probability of being in
  # the last context in which you were rewarded
  rw_idx <- c(with(tmp, which(diff(t) != 0)), nrow(tmp)) # get the row numbers where each trial ends, this will allow me to pull out the context for each trial
  cntx <- with(tmp, context[rw_idx]) # get the pattern of context changes
  # now I need to count, for each trial, what is the probability that its the same as the last trial
  cntx_chngs <- diff(cntx) # get whether you were in the same or differeny context on the previous trial (this gives 2:end
  cntx_chngs <- !cntx_chngs # make the stays a true, and the changes a false
  psC <- c(.5, .5) # the probability you are in the same context as the last one you saw (pdC will be 1 - this number). p=.5 reflects uninformed priors, 2 values as the first update comes at the end of trial 2.
  for (i in 1:(length(cntx_chngs)-1)){ # we only go to the penultimate observation as there are no updates after the last trial 
    psC <- c(psC, sum(cntx_chngs[1:i])/i) # at the end of each trial, I evaluate the probability of a context being the same as the one I were last rewarded in. This will get fed into the next trial as the estimate of the probability of being in the same context as the last rewarded one.
  }
  ts <- with(tmp, unique(t))
  tmp <- inner_join(tmp, tibble(t = ts, psC = psC, pdC = 1 - psC), by="t") 
 
  ####################################
  
  ######################################
  # now I need for each trial, the number of n's (fails from the same context as the previous), and the number of m's (fails from the different context as the previous). A 'fail' is a selection of a task relevant door, and the failure to find a target
  
  # first, get the previously rewarded context
  tmp <- inner_join(tmp, tibble(t = unique(ts), 
                                prv_cntx = c(0, cntx[1:(length(ts)-1)])),
                    by="t")
  
  # we need to assign an appropriate 'previous context' to trial 1, so lets take the context
  # from which they first select a target door
  cntx_a_tgts <- unique(tmp$door[tmp$context == 1 & tmp$door_cc > 0]) ## how does this not return 4 values?
  cntx_b_tgts <- unique(tmp$door[tmp$context == 2 & tmp$door_cc > 0])
  tgts <- matrix(c(cntx_a_tgts, cntx_b_tgts), ncol=2)
  
  frst_tgt_door <- tmp$door[which(tmp$door_cc == 1 | tmp$door_oc == 1)[1]]
  prv_cntxt_4_frst_trl <- which(tgts == frst_tgt_door, arr.ind=T)[,'col']
  tmp$prv_cntx[tmp$t == 1] = prv_cntxt_4_frst_trl
  ## now I need to recode cc and oc responses as following:
  ## if current context == 1 & prev context == 1, n = cc, m = oc
  ## if current context == 1 & prev context == 2, n = oc, m = cc
  ## if current context == 2 & prev context == 2, n = cc, m = oc
  ## if current context == 2 & prev context == 1, n = oc, m = cc
  tmp$door_n <- 0
  tmp$door_m <- 0 #####NOTE: door_m is also our DV aka we are modelling responses that
  # are from the context in which you were not just rewarded
  tmp$door_n[tmp$context == tmp$prv_cntx] <- 
    tmp$door_cc[tmp$context == tmp$prv_cntx]
  # row 1 & 3, [n] from comments above
  tmp$door_n[tmp$context != tmp$prv_cntx] <- 
    tmp$door_oc[tmp$context != tmp$prv_cntx] # row 1 & 3 [m] from comments above. 
  tmp$door_m[tmp$context == tmp$prv_cntx] <- # 
    tmp$door_oc[tmp$context == tmp$prv_cntx] # row 1 & 3 [m]
  tmp$door_m[tmp$context != tmp$prv_cntx ] <- 
    tmp$door_cc[tmp$context != tmp$prv_cntx] # row 2 & 4 [m]
  
  ## now I need to add to this bit a filtering of the unique target selections on that trial, 
  ## and remove the n's and m's which are repetitions

  remove_repeats <- function(tmp, trialN, tgts){
    ###########
  # here I write a function where for each trial I take the door_n selections (i.e. the relevant ones, relative to last reward). I find which of them are repetitions, and I remove those repetitions
    ##########
    trial_dat <- tmp %>% filter(t == trialN)
    
    ##########
    # first deal with the ns
    tgts_tn <- tgts[,trial_dat$prv_cntx[1]] # what were the n targets on this trial
    tidx <- which(trial_dat$door_n > 0) # on which rows did someone select an n door?
    scnd_ns_this_trl <- duplicated(trial_dat$door[tidx]) # find which selections are  repeat visits
    # get the idx for which ones should be removed
    trial_dat$door_frst_n <- trial_dat$door_n # establish the required variable
    trial_dat$door_frst_n[tidx[scnd_ns_this_trl]] <- 0
    
    ##########
    # now deal with the ms - code is a little clunky. I may generalise this later
    tgts_tm <- tgts[,3-trial_dat$prv_cntx[1]]
    midx <- which(trial_dat$door_m > 0)
    scnd_ms_this_trl <- duplicated(trial_dat$door[midx])
    trial_dat$door_frst_m <- trial_dat$door_m # establish the required variable
    trial_dat$door_frst_m[midx[scnd_ms_this_trl]] <- 0
    
    ########## 
    # now what we actually need to do is shift the first_n variables down 1, and put 0 in the first
    # selection of the trial. This is because we need the regressor to reflect the update that 
    # influences perfomance on that trial - e.g. trial 2 information affects trial 3, not trial 2
    nr = nrow(trial_dat)
    if (nr > 1){
      trial_dat$door_frst_n <- c(0, trial_dat$door_frst_n[1:(nr-1)]) # because we won't update after 
      # we've found the target
      trial_dat$door_frst_m <- c(0, trial_dat$door_frst_m[1:(nr-1)])
    } else {
      trial_dat$door_frst_n <- 0
      trial_dat$door_frst_m <- 0
    }

    ###########
    # return the new dataframe
    trial_dat
  }
  
  trls <- unique(tmp$t)
  tmp <- do.call(rbind, lapply(trls, remove_repeats, tmp=tmp, tgts=tgts))
      
################################################################
  
#############################################################
  # next, remove the last door selection from each trial, as that was a hit (not a null)
  tmp <- tmp[-rw_idx,]
  
  ## now by trial, compute the cumulative sum of door_n and door_m
  tmp <- tmp %>% group_by(t) %>% mutate(sum_n = cumsum(door_frst_n),
                                        sum_m = cumsum(door_frst_m)) 
#############################################################
  
#############################################################
  # next, I need a column giving the probability of n or m nulls, given you are
  # in the previously rewarded context (for n), or the other context (for m)
  p_vals <- c(1, .75, .5, .25, 0)
  tmp$p_n_g_sC <- p_vals[as.factor(tmp$sum_n)] # probability of n, given you are in the same context as where you were last rewarded
  tmp$p_m_g_dC <- p_vals[as.factor(tmp$sum_m)] # probability of m, given you are in the different context from where you were last rewarded
  tmp$p_n_g_sC[is.na(tmp$p_n_g_sC)] <- 0
  tmp$p_m_g_dC[is.na(tmp$p_m_g_dC)] <- 0
#############################################################

#############################################################
  ##### so what I have now on each row is the probability you are in the same context, given your 
  ##### previous experience. I also have the number of n or m you have selected, up to the previous 
  ##### trial
  ##### so the info on each row reflects the information available to influence behaviour on that
  ##### trial (i.e. the info from 1:t-1)
  ##### so the I can compute on each trial, the probability of being in the same context given that
  ##### many ns and ms that have been discovered up until now
  ##### I will then substract that from 1, to get the probability you are in 
  ##### the other context. This will help with interpretability.
  tmp <- tmp %>% mutate(N_oc = 1-(p_n_g_sC*psC /  (p_n_g_sC*psC + p_m_g_dC*pdC)))
#############################################################  
  tmp
}
```

```{r}
dat <- get_p_context(dat, subN = subN)

N_oc_p <- dat %>% ggplot(aes(x=t, y=N_oc)) + geom_line()
```

Okies, now I have all the regressors, I am going to do a pairs plot to check for correlations between the predictors

```{r}

dat <- dat %>% mutate(Sw_r = Sw/t) # first, get the switch rate as a permanent variable
```

```{r, warning=FALSE, echo=FALSE}

ggpairs(dat %>% select(Sw, Sw_r, N_oc))
```

All of the correlations are tolerable for subject 3. Correlations between Sw and Sw_r can be high. Might need to prefer one hypothesis over the other for the model. Will try a logistic regression model first to see.

```{r}

fit <- glm(door_m ~ Sw + Sw_r + N_oc , data=dat, family=binomial(link="logit"))
summary(fit)
```

## Understanding correlations between regressors

I am going to plot all the regressors together so I can understand how and why they are sharing so much information.

```{r}

grid.arrange(Sw_p, Sw_r_p, N_oc_p, nrow=2)
```

Number of switches and estimate of switch rate are correlated. The estimate goes down as there is a long delay between switches, but it is not hard to see how they track each other.

Tgt N is largely tracking the same as sw_r, as in the sw_r will be determined by the number of doors occurring in between, given the dependence on trial number. So **remove TgtN from the modelling.**
